{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# To load a pretrained model and a tokenizer using HuggingFace we only need two line of code\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Let's test the model\n",
    "text = \"Udacity is the best place to learn about generative\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Show the tokens as numbers, i.e. \"input_ids\"\n",
    "print(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Examine the tokenization\n",
    "### Let's explore what tokens mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(52)</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(67)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(4355)</td>\n",
       "      <td>acity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(318)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(1266)</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(1295)</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(284)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(2193)</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(546)</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(1152)</td>\n",
       "      <td>gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(876)</td>\n",
       "      <td>ative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   token\n",
       "0     tensor(52)       U\n",
       "1     tensor(67)       d\n",
       "2   tensor(4355)   acity\n",
       "3    tensor(318)      is\n",
       "4    tensor(262)     the\n",
       "5   tensor(1266)    best\n",
       "6   tensor(1295)   place\n",
       "7    tensor(284)      to\n",
       "8   tensor(2193)   learn\n",
       "9    tensor(546)   about\n",
       "10  tensor(1152)   gener\n",
       "11   tensor(876)   ative"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show hoe the sentence looks like\n",
    "import pandas as pd\n",
    "\n",
    "def show_tokenization(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
    "        columns=[\"id\", \"token\"],\n",
    "    )\n",
    "    \n",
    "show_tokenization(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Calculate the probabilty of the next token\n",
    "Now lets use PyTorch to calculate the probability of the next token given the previous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>8300</td>\n",
       "      <td>programming</td>\n",
       "      <td>0.157587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>4673</td>\n",
       "      <td>learning</td>\n",
       "      <td>0.148417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>4981</td>\n",
       "      <td>models</td>\n",
       "      <td>0.048505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>17219</td>\n",
       "      <td>biology</td>\n",
       "      <td>0.046481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16113</th>\n",
       "      <td>16113</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>0.027796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id         token         p\n",
       "8300    8300   programming  0.157587\n",
       "4673    4673      learning  0.148417\n",
       "4981    4981        models  0.048505\n",
       "17219  17219       biology  0.046481\n",
       "16113  16113    algorithms  0.027796"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "\n",
    "def show_next_token_choices(probabilities, top_n=5):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns=[\"id\", \"token\", \"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrresting! The model thinks that the most likely next work is  \"programming\" followed up colesly by \"learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 8300\n",
      "Next token:  programming\n"
     ]
    }
   ],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Udacity is the best place to learn about generative programming'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We append the most likely next token to the input\n",
    "text =  text + tokenizer.decode(8300)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Now lets generate meore tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udacity is the best place to learn about generative programming\n",
      "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876,\n",
      "         8300]])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Next token probabilities:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>0.352228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>,</td>\n",
       "      <td>0.135984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>and</td>\n",
       "      <td>0.109371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>in</td>\n",
       "      <td>0.069530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>8950</td>\n",
       "      <td>languages</td>\n",
       "      <td>0.058288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       token         p\n",
       "13      13           .  0.352228\n",
       "11      11           ,  0.135984\n",
       "290    290         and  0.109371\n",
       "287    287          in  0.069530\n",
       "8950  8950   languages  0.058288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udacity is the best place to learn about generative programming.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# show the text\n",
    "print(text)\n",
    "\n",
    "# convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "# Calculate the probabilities for the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token id and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Use the generate method\n",
    "# Start with some text and tokenize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use the gerneragte method\n",
    "### Start with some text and tokenize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One upon a time, generative models were used to predict the future. In the early 20th century, the first generative models were developed to predict the future. The first generative models were based on the idea that the future is a series of events, and that the future is a series of events. The first generative models were based on the idea that the future is a series of events, and that the future is a series of events. The first generative models were based on"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stqart with some text and tokenize it\n",
    "text = \"One upon a time, generative models were\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# use the 'generate method' to generate the next token\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
