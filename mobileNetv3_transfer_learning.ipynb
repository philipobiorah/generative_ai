{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Transfer learning using MobileNetV3\n",
    "\n",
    "In the field of machine learning, transfer learning is a powerful technique that leverages pre-trained models and applies them to new tasks. This approach allows us to save time and computational resources by reusing the knowledge gained from training on large datasets.\n",
    "\n",
    "In this exercise we use MobileNetV3, a convolutional neural network architecture for mobile devices, to train a classifier for the Fashion-MNIST dataset using the PyTorch framework.\n",
    "\n",
    "Fashion-MNIST is a drop-in replacement for MNIST (images of size 28x28 with 10 classes) but instead of hand-written digits it contains tiny images of clothes!\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Load the Fashion-MNIST dataset using the torchvision package.\n",
    "2. Define a PyTorch model using the MobileNetV3 architecture.\n",
    "3. Train the model on the Fashion-MNIST dataset.\n",
    "4. Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Fashion-MNIST dataset\n",
    "\n",
    "The torchvision package provides access to popular datasets, model architectures, and image transformations for computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Fashion-MNIST dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_ctx' from 'torch.library' (/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/library.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torchvision/_meta_registrations.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_custom_ops\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure that torch.ops.torchvision is visible\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_custom_ops.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_custom_op\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     _custom_op_with_schema,\n\u001b[1;32m      6\u001b[0m     _find_custom_op,\n\u001b[1;32m      7\u001b[0m     infer_schema,\n\u001b[1;32m      8\u001b[0m     parse_qualname,\n\u001b[1;32m      9\u001b[0m     validate_namespace,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ctx\n\u001b[1;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_op\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m ]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_custom_op/impl.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_C\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlibrary\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ctx\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograd_kernel_indirection, construct_autograd_kernel\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_library\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer_schema\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_ctx' from 'torch.library' (/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/library.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(batch_size, data_dir=\"data\"):\n",
    "    \"\"\"Load the Fashion-MNIST dataset.\"\"\"\n",
    "\n",
    "    # Define transforms to normalize the data\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    # Download and load the training data\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        data_dir, download=True, train=True, transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Download and load the test data\n",
    "    testset = datasets.FashionMNIST(\n",
    "        data_dir, download=True, train=False, transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainloader, testloader \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;241m64\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(batch_size, data_dir)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the Fashion-MNIST dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define transforms to normalize the data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      6\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mToTensor(), transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))]\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Download and load the training data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m trainset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mFashionMNIST(\n\u001b[1;32m     11\u001b[0m     data_dir, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = load_data(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_index=0, class_name=T-shirt/top\n",
      "class_index=1, class_name=Trouser\n",
      "class_index=2, class_name=Pullover\n",
      "class_index=3, class_name=Dress\n",
      "class_index=4, class_name=Coat\n",
      "class_index=5, class_name=Sandal\n",
      "class_index=6, class_name=Shirt\n",
      "class_index=7, class_name=Sneaker\n",
      "class_index=8, class_name=Bag\n",
      "class_index=9, class_name=Ankle boot\n"
     ]
    }
   ],
   "source": [
    "# Define some helper functions to helps with the labels\n",
    "def get_class_names():\n",
    "    \"\"\"Return the list of classes in the Fashion-MNIST dataset.\"\"\"\n",
    "    return [\n",
    "        \"T-shirt/top\",\n",
    "        \"Trouser\",\n",
    "        \"Pullover\",\n",
    "        \"Dress\",\n",
    "        \"Coat\",\n",
    "        \"Sandal\",\n",
    "        \"Shirt\",\n",
    "        \"Sneaker\",\n",
    "        \"Bag\",\n",
    "        \"Ankle boot\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_class_name(class_index):\n",
    "    \"\"\"Return the class name for the given index.\"\"\"\n",
    "    return get_class_names()[class_index]\n",
    "\n",
    "\n",
    "def get_class_index(class_name):\n",
    "    \"\"\"Return the class index for the given name.\"\"\"\n",
    "    return get_class_names().index(class_name)\n",
    "\n",
    "\n",
    "for class_index in range(10):\n",
    "    print(f\"class_index={class_index}, class_name={get_class_name(class_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 images from the training set with their labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()  # convert from tensor to numpy array\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # transpose dimensions\n",
    "\n",
    "\n",
    "images, labels = next(iter(trainloader))  # get the first batch\n",
    "\n",
    "# show images with labels\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plot_size = 10\n",
    "\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size // 2, idx + 1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(get_class_name(int(labels[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained MobileNetV3 and inspect its structure\n",
    "import torchvision.models as models\n",
    "\n",
    "mobilenet_v3_model = models.mobilenet_v3_small(pretrained=True)\n",
    "print(mobilenet_v3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <MASK> to complete this code cell\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Define a model class that extends the nn.Module class\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        num_classes = 10\n",
    "        # Load the pre-trained MobileNetV3 (Small) architecture\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        # Replace the last fully-connected layer with a new one of the right size\n",
    "        self.model.classifier[3] = nn.Linear(self.model.classifier[3].in_features, num_classes)\n",
    "\n",
    "        # Freeze all the weights of the network except for the last fully-connected layer\n",
    "        self.freeze()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert 1x28x28 input tensor to 3x28x28 tensor, to convert it to a color image\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Resize the input to 224x224, since MobileNetV3 (Small) expects images of that size\n",
    "        if x.shape[2:] != (224, 224):\n",
    "            x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Forward pass\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze(self):\n",
    "        # Freeze all the weights of the network except for the last fully-connected layer\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the final layer\n",
    "        for param in self.model.classifier[3].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all the weights of the network\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "# Create an instance of the MobileNetV3 model\n",
    "model = MobileNetV3()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device as GPU, MPS, or CPU according to availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <MASK> to complete this code cell\n",
    "\n",
    "# Create a PyTorch training loop\n",
    "\n",
    "# Create a PyTorch training loop\n",
    "\n",
    "model = model.to(device)  # Move the model weights to the device\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for batch_num, (images, labels) in enumerate(trainloader):\n",
    "        # Move tensors to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss and perform backprop\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss for every 100th iteration\n",
    "        if (batch_num) % 100 == 0:\n",
    "            print(\n",
    "                \"Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}\".format(\n",
    "                    epoch + 1, epochs, batch_num + 1, len(trainloader), loss.item()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate the model on the test set\n",
    "\n",
    "We evaluate the model on the test set by:\n",
    "* printing the accuracy\n",
    "* plotting a few examples of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m testloader:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Move tensors to the configured device\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace <MASK> to complete this code cell\n",
    "\n",
    "# Print the loss and accuracy on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "loss = 0\n",
    "\n",
    "for images, labels in testloader:\n",
    "    # Move tensors to the configured device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss += loss_fn(outputs, labels).item()\n",
    "\n",
    "    # torch.max return both max and argmax. We get the argmax here.\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\n",
    "    \"Test Accuracy of the model on the test images: {} %\".format(100 * correct / total)\n",
    ")\n",
    "print(\"Test Loss of the model on the test images: {}\".format(loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
